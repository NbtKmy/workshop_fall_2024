{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Einfaches Chatbot erstellen!\n",
        "\n",
        "In diesem WS wollen wir ein einfaches Chatbot in der Colaboratory-Umgebung erstellen!\n",
        "\n",
        "Zuerst schauen wir\n",
        "\n",
        "\n",
        "https://cookbook.openai.com/examples/gpt4o/introduction_to_gpt4o"
      ],
      "metadata": {
        "id": "6rG-WG7sYw1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSMHVoBiUAXE",
        "outputId": "fef21a40-12a6-44c8-bcd4-1c29034ac2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m378.9/383.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API-Token eintragen und Model auswählen\n",
        "\n"
      ],
      "metadata": {
        "id": "G7673tbVZoMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "MODEL=\"gpt-4o-mini\"\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "jZt5ZJmxUc9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ganz simple Frage und Antwort\n",
        "\n",
        "Nach [Instruktion von OpenAI](https://platform.openai.com/docs/guides/chat-completions/getting-started) sieht das ganz einfache Chat so aus:\n",
        "\n",
        "```\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]\n",
        ")\n",
        "```\n",
        "\n",
        "Es sind noch weitere Parameters, die für uns interessant sind... Falls man noch dies eingeben möchte, kann man [hier](https://platform.openai.com/docs/api-reference/chat/create) ansehen. Für uns wären z.B. temperature und top-p interessant:\n",
        "\n",
        "```\n",
        "Temperature\n",
        "\n",
        "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
        "We generally recommend altering this or top_p but not both.\n",
        "```\n",
        "\n",
        "```\n",
        "top_p\n",
        "\n",
        "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
        "We generally recommend altering this or temperature but not both.\n",
        "```\n",
        "\n",
        "Sonst kann man natürlich über das Systemprompt Gedanken machen!\n",
        "Für [Claude (Anthropic)](https://docs.anthropic.com/en/release-notes/system-prompts#sept-9th-2024) wurde System Prompts bekannt gegeben. Vielleicht hilft es...\n",
        "\n"
      ],
      "metadata": {
        "id": "ojVTYg6YZyDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "system_prompt = {\"role\": \"system\", \"content\": \"Du gibst als ein freundlicher Ratgeber konstruktive Ratschläge. Du sprichst nur auf Deutsch\"}\n",
        "messages.append(system_prompt)\n",
        "\n",
        "human_input = input ()\n",
        "messages.append({\"role\": \"user\", \"content\": human_input})\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  temperature=0.9,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(\"Assistant: \" + completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7uGXVluVK90",
        "outputId": "7cef31b3-6ca5-4e57-818d-98142cd8d245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hallo\n",
            "Assistant: Hallo! Wie kann ich dir helfen?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatBot mit Chat-Historie?\n",
        "\n",
        "Wir sind jetzt in der Lage, eine Antwort von ChatGPT zu holen...\n",
        "Aber wenn wir von einem ChatBot reden, stellen wir uns normalerweise vor, dass das ChatBot den Inhalt des vorangehenden Gesprächs merkt und das Gespräch weiterführt.\n",
        "\n",
        "Wie kann man das realisieren?\n",
        "Man könnte dafür [Assistants API (Beta)](https://platform.openai.com/docs/assistants/quickstart/agents) verwenden, aber hier zuerst bleiben wir bei \"[Chat Completions](https://platform.openai.com/docs/guides/chat-completions)\".\n",
        "Chat Completions ist stateless - D.h., dass Chat-Historie im Parameter \"messages\" mitgegeben werden muss...\n",
        "\n",
        "\n",
        "...Wenn man damit fertig ist, kann man überlegen, wie man ein schönes UI dazu basteln kann. Es gibt dafür eine praktische Library [Gradio](https://www.gradio.app/)..."
      ],
      "metadata": {
        "id": "IDzJ1LMbbd4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ib4BW5kCZ-cX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlrR8dQtpUSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
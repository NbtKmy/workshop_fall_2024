{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNXv0gVV6ZKDZ35UKINPagB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22dcb0402db548ee82ce9839651a376e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26f119472006413e809543d1efdb167f","IPY_MODEL_fbee45f974394f10866acf4357704a8e","IPY_MODEL_91fe23a50a514f68b39526e8df936171"],"layout":"IPY_MODEL_f264fb3e7b2140c0bd8cfe2e042cd2f1"}},"26f119472006413e809543d1efdb167f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dd65309385a4fc3b830a9f08dc4f1b3","placeholder":"​","style":"IPY_MODEL_ca1c0273bcee4a32a0ff707043af4040","value":"llama-3.2-3b-instruct-q4_k_m.gguf: 100%"}},"fbee45f974394f10866acf4357704a8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c3e5066540149f282595dfac554c23b","max":2019373920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fe68936f9d544879d1a8fa0a2949fe4","value":2019373920}},"91fe23a50a514f68b39526e8df936171":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b436ad5eae4f6a9f8061d52dccb10f","placeholder":"​","style":"IPY_MODEL_403c1c1d3dae48d4a26a70703482d015","value":" 2.02G/2.02G [00:06&lt;00:00, 327MB/s]"}},"f264fb3e7b2140c0bd8cfe2e042cd2f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dd65309385a4fc3b830a9f08dc4f1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca1c0273bcee4a32a0ff707043af4040":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c3e5066540149f282595dfac554c23b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fe68936f9d544879d1a8fa0a2949fe4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5b436ad5eae4f6a9f8061d52dccb10f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403c1c1d3dae48d4a26a70703482d015":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install llama-cpp-python --upgrade --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvJJgzas1D6n","executionInfo":{"status":"ok","timestamp":1729023891212,"user_tz":-120,"elapsed":66512,"user":{"displayName":"Nobutake Kamiya","userId":"02941915511556130991"}},"outputId":"4918103e-15f6-4362-eb06-a14dbc589bea"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m237.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.1)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3510526 sha256=a90c97a37be08885d39bd7521714cfed2d4c1ee8446fb9bb5aa11495cdd399e4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ymjxqo2d/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\n"]}]},{"cell_type":"code","source":["from llama_cpp import Llama"],"metadata":{"id":"eTCm5NoqfxGj","executionInfo":{"status":"ok","timestamp":1729023909828,"user_tz":-120,"elapsed":307,"user":{"displayName":"Nobutake Kamiya","userId":"02941915511556130991"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["llm = Llama.from_pretrained(\n","\trepo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n","\tfilename=\"llama-3.2-3b-instruct-q4_k_m.gguf\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["22dcb0402db548ee82ce9839651a376e","26f119472006413e809543d1efdb167f","fbee45f974394f10866acf4357704a8e","91fe23a50a514f68b39526e8df936171","f264fb3e7b2140c0bd8cfe2e042cd2f1","6dd65309385a4fc3b830a9f08dc4f1b3","ca1c0273bcee4a32a0ff707043af4040","7c3e5066540149f282595dfac554c23b","3fe68936f9d544879d1a8fa0a2949fe4","b5b436ad5eae4f6a9f8061d52dccb10f","403c1c1d3dae48d4a26a70703482d015"]},"id":"MJxjUNjVz2TB","executionInfo":{"status":"ok","timestamp":1729023922104,"user_tz":-120,"elapsed":9700,"user":{"displayName":"Nobutake Kamiya","userId":"02941915511556130991"}},"outputId":"001162db-e7a3-4d03-be3b-f67b79993f9e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["llama-3.2-3b-instruct-q4_k_m.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22dcb0402db548ee82ce9839651a376e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /root/.cache/huggingface/hub/models--hugging-quants--Llama-3.2-3B-Instruct-Q4_K_M-GGUF/snapshots/eb72f2a08dd2b9edd07ffacfe5aa56938b7939b0/./llama-3.2-3b-instruct-q4_k_m.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n","llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n","llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n","llama_model_loader: - kv   5:                         general.size_label str              = 3B\n","llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n","llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n","llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n","llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n","llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n","llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n","llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n","llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n","llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n","llama_model_loader: - kv  18:                          general.file_type u32              = 15\n","llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n","llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n","llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n","llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   58 tensors\n","llama_model_loader: - type q4_K:  168 tensors\n","llama_model_loader: - type q6_K:   29 tensors\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.7999 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 131072\n","llm_load_print_meta: n_embd           = 3072\n","llm_load_print_meta: n_layer          = 28\n","llm_load_print_meta: n_head           = 24\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 3\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 8192\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 131072\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = ?B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 3.21 B\n","llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \n","llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n","llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n","llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: ggml ctx size =    0.12 MiB\n","llm_load_tensors:        CPU buffer size =  1918.35 MiB\n","...........................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =    56.00 MiB\n","llama_new_context_with_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n","llama_new_context_with_model: graph nodes  = 902\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '24', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.block_count': '28', 'llama.attention.head_count_kv': '8'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n","\n","'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n","\n","' }}\n","Using chat eos_token: <|eot_id|>\n","Using chat bos_token: <|begin_of_text|>\n"]}]},{"cell_type":"code","source":["output = llm.create_chat_completion(\n","\tmessages = [\n","\t\t{\n","\t\t\t\"role\": \"user\",\n","\t\t\t\"content\": \"What is quantization of the LLM?\"\n","\t\t}\n","\t]\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zncspnr51juS","executionInfo":{"status":"ok","timestamp":1729024003068,"user_tz":-120,"elapsed":32368,"user":{"displayName":"Nobutake Kamiya","userId":"02941915511556130991"}},"outputId":"2f977b07-b968-4107-e3f8-c7840848fe7d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_perf_context_print:        load time =     528.65 ms\n","llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:        eval time =       0.00 ms /   492 runs   (    0.00 ms per token,      inf tokens per second)\n","llama_perf_context_print:       total time =   32069.17 ms /   511 tokens\n"]},{"output_type":"stream","name":"stdout","text":["{'role': 'assistant', 'content': \"A very interesting topic!\\n\\nQuantization of a Large Language Model (LLM) refers to the process of reducing the computational resources required to run a large neural network, such as a LLM, by converting its weights and activations to a lower precision data type.\\n\\nIn traditional floating-point computing, LLMs are typically implemented using 32-bit or 16-bit floating-point numbers, which can store values with a very high degree of precision. However, these high-precision representations come at a cost:\\n\\n1. **Increased memory usage**: More bits mean more memory is required to store the weights and activations, which can lead to larger model sizes and increased memory usage.\\n2. **Increased computational overhead**: Floating-point operations are generally slower and more expensive than integer arithmetic.\\n\\nQuantization reduces the precision of the model's weights and activations from floating-point to lower-precision integers, such as:\\n\\n* **Fixed-point**: Weights and activations are represented using fixed-point numbers, with a fixed number of bits for the mantissa (fractional part) and an integer exponent.\\n* **Integer quantization**: Weights and activations are represented using integers, with a fixed number of bits for each component.\\n* **Binary quantization**: Weights and activations are represented using binary values (0s and 1s) with some additional precision.\\n\\nBy reducing the precision of the model, quantization:\\n\\n1. **Reduces memory usage**: Smaller weights and activations require less memory, making the model more efficient and easier to store.\\n2. **Simplifies computations**: Integer arithmetic is generally faster and less expensive than floating-point operations, leading to faster inference and training.\\n3. **Improves hardware efficiency**: Many modern hardware platforms, such as GPUs and TPUs, are designed to handle integer arithmetic more efficiently than floating-point operations.\\n\\nHowever, quantization can also lead to some trade-offs, such as:\\n\\n1. **Loss of precision**: Reducing the precision of the model's weights and activations can lead to decreased model accuracy and performance.\\n2. **Increased risk of overflow and underflow**: With reduced precision, the model may be more susceptible to overflow or underflow errors, especially when dealing with large or very small values.\\n\\nTo mitigate these trade-offs, researchers and practitioners use various quantization techniques, such as:\\n\\n1. **Weight quantization**: Only quantizing the weights, leaving the activations at full precision.\\n2. **Activation quantization**: Only quantizing\"}\n"]}]},{"cell_type":"code","source":["print(output[\"choices\"][0][\"message\"][\"content\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAgdqTLJ3RUy","executionInfo":{"status":"ok","timestamp":1729024050132,"user_tz":-120,"elapsed":257,"user":{"displayName":"Nobutake Kamiya","userId":"02941915511556130991"}},"outputId":"b7e36c19-6f3e-4662-94d0-45a3edaa0e2d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["A very interesting topic!\n","\n","Quantization of a Large Language Model (LLM) refers to the process of reducing the computational resources required to run a large neural network, such as a LLM, by converting its weights and activations to a lower precision data type.\n","\n","In traditional floating-point computing, LLMs are typically implemented using 32-bit or 16-bit floating-point numbers, which can store values with a very high degree of precision. However, these high-precision representations come at a cost:\n","\n","1. **Increased memory usage**: More bits mean more memory is required to store the weights and activations, which can lead to larger model sizes and increased memory usage.\n","2. **Increased computational overhead**: Floating-point operations are generally slower and more expensive than integer arithmetic.\n","\n","Quantization reduces the precision of the model's weights and activations from floating-point to lower-precision integers, such as:\n","\n","* **Fixed-point**: Weights and activations are represented using fixed-point numbers, with a fixed number of bits for the mantissa (fractional part) and an integer exponent.\n","* **Integer quantization**: Weights and activations are represented using integers, with a fixed number of bits for each component.\n","* **Binary quantization**: Weights and activations are represented using binary values (0s and 1s) with some additional precision.\n","\n","By reducing the precision of the model, quantization:\n","\n","1. **Reduces memory usage**: Smaller weights and activations require less memory, making the model more efficient and easier to store.\n","2. **Simplifies computations**: Integer arithmetic is generally faster and less expensive than floating-point operations, leading to faster inference and training.\n","3. **Improves hardware efficiency**: Many modern hardware platforms, such as GPUs and TPUs, are designed to handle integer arithmetic more efficiently than floating-point operations.\n","\n","However, quantization can also lead to some trade-offs, such as:\n","\n","1. **Loss of precision**: Reducing the precision of the model's weights and activations can lead to decreased model accuracy and performance.\n","2. **Increased risk of overflow and underflow**: With reduced precision, the model may be more susceptible to overflow or underflow errors, especially when dealing with large or very small values.\n","\n","To mitigate these trade-offs, researchers and practitioners use various quantization techniques, such as:\n","\n","1. **Weight quantization**: Only quantizing the weights, leaving the activations at full precision.\n","2. **Activation quantization**: Only quantizing\n"]}]}]}